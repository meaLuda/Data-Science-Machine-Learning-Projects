{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Multivariate Analysis: Discriminant Analysis","provenance":[{"file_id":"1TwmGdHNfUEA1H5sClk_yNxJbyDCpG4Ka","timestamp":1635327717091}],"collapsed_sections":["GUgBAM8Gml_r","VkRpYg_0mpBI"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ov3KXBftKymB"},"source":["<font color=\"green\">*To start working on this notebook, or any other notebook that we will use in the Moringa Data Science Course, we will need to save our own copy of it. We can do this by clicking File > Save a Copy in Drive. We will then be able to make edits to our own copy of this notebook.*</font>"]},{"cell_type":"markdown","metadata":{"id":"h3q8mXwWNQhL"},"source":["# Multivariate Analysis with Python: Linear Discriminant Analysis"]},{"cell_type":"markdown","metadata":{"id":"diwV5qxcm1lk"},"source":["## Linear Discriminant Analysis"]},{"cell_type":"markdown","metadata":{"id":"Hq1AAPr9DkBB"},"source":["**Linear Discriminant Analysis (LDA)** is a simple and powerful linear transformation that is most commonly used as dimensionality reduction technique in the pre-processing step for machine learning applications. The goal of linear discriminant analysis is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\"curse of dimensionality\") and also reduce computational costs. \n","\n","To further explain dimensionality reduction; \n","\n","> Dimensionality Reduction is a machine learning or statistical technique of reducing the amount of random variables in a problem by obtaining a set of principal variables. This process can be carried out using a number of methods that simplify the modeling of complex problems, eliminate redundancy and reduce the possibility of the model overfitting and thereby including results that do not belong. \n","\n","The following link provides more explanation of dimensionality reduction ([Link](https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/)). "]},{"cell_type":"markdown","metadata":{"id":"GUgBAM8Gml_r"},"source":["### Example 1"]},{"cell_type":"code","metadata":{"id":"EBQAN8iHlELB"},"source":["# Step 1: Importing the libraries that we will need \n","#\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lo8KHVE7lKys","colab":{"base_uri":"https://localhost:8080/","height":205},"executionInfo":{"status":"ok","timestamp":1635327774599,"user_tz":-180,"elapsed":1507,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}},"outputId":"d5e5cd8c-714e-4d4a-9ac9-5b0eaaaeb892"},"source":["# Step 2: Loading the Dataset\n","#\n","dataset = pd.read_csv(\"http://bit.ly/IrisDataset\")\n","dataset.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>Iris-setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal_length  sepal_width  petal_length  petal_width      species\n","0           5.1          3.5           1.4          0.2  Iris-setosa\n","1           4.9          3.0           1.4          0.2  Iris-setosa\n","2           4.7          3.2           1.3          0.2  Iris-setosa\n","3           4.6          3.1           1.5          0.2  Iris-setosa\n","4           5.0          3.6           1.4          0.2  Iris-setosa"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"TwPDJdbelPRm","executionInfo":{"status":"ok","timestamp":1635327807134,"user_tz":-180,"elapsed":591,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Step 3: Once dataset is loaded into a pandas data frame object, the first step is to divide dataset \n","# into features and corresponding labels and then divide the resultant dataset into training and test sets. \n","# The following code divides data into labels and feature set. \n","# The code assigns the first four columns of the dataset i.e. the feature set to X variable \n","# while the values in the fifth column (labels) are assigned to the y variable.\n","#\n","X = dataset.iloc[:, 0:4].values\n","y = dataset.iloc[:, 4].values"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ojxzyAj5lmuE","executionInfo":{"status":"ok","timestamp":1635327811857,"user_tz":-180,"elapsed":517,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Step 4: The following code divides data into training and test sets\n","#\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMBU96JaluA9","executionInfo":{"status":"ok","timestamp":1635327827567,"user_tz":-180,"elapsed":460,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Step 5: Feature scaling\n","# We now need to perform feature scaling. We execute the following code to do so:\n","# \n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"riC3vs3fmBX6","executionInfo":{"status":"ok","timestamp":1635327907893,"user_tz":-180,"elapsed":436,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Step 6: Peforming LDA\n","# It requires only four lines of code to perform LDA with Scikit-Learn. \n","# The LinearDiscriminantAnalysis class of the sklearn.discriminant_analysis \n","# library can be used to Perform LDA in Python. \n","# Let's take a look at the following code\n","#\n","\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","lda = LDA(n_components=1)\n","X_train = lda.fit_transform(X_train, y_train)\n","X_test = lda.transform(X_test)\n","# In the script above the LinearDiscriminantAnalysis class is imported as LDA. \n","# We have to pass the value for the n_components parameter of the LDA, \n","# which refers to the number of linear discriminates that we want to retrieve. \n","# In this case we set the n_components to 1, since we first want to check the performance \n","# of our classifier with a single linear discriminant. \n","# Finally we execute the fit and transform methods to actually retrieve the linear discriminants.\n","# Notice, in case of LDA, the transform method takes two parameters: the X_train and the y_train. \n","# This reflects the fact that LDA takes the output class labels into account while selecting the linear discriminants."],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"dbgQysmuXjQr","executionInfo":{"status":"ok","timestamp":1635327951814,"user_tz":-180,"elapsed":410,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Step 7: Training and Making Predictions\n","# We will use the random forest classifier to evaluate the performance of a PCA-reduced algorithms as shown\n","# \n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","classifier = RandomForestClassifier(max_depth=2, random_state=0)\n","classifier.fit(X_train, y_train)\n","y_pred = classifier.predict(X_test)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTU-I0PoX-ds","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635327987129,"user_tz":-180,"elapsed":418,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}},"outputId":"2f05f980-cdda-4f96-ed90-392a29ee4b00"},"source":["# Step 8: Evaluating the Performance\n","# As always, the last step is to evaluate performance of the algorithm \n","# with the help of a confusion matrix and find the accuracy of the prediction.\n","# \n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","print('Accuracy' + str(accuracy_score(y_test, y_pred)))\n","\n","# We can see that with one linear discriminant, the algorithm achieved an accuracy of 100%, \n","# which is greater than the accuracy achieved with one principal component, which was 93.33%."],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[11  0  0]\n"," [ 0 13  0]\n"," [ 0  0  6]]\n","Accuracy1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"VkRpYg_0mpBI"},"source":["### <font color=\"green\">Challenges</font>"]},{"cell_type":"code","metadata":{"id":"hzcNwYp2mtco","colab":{"base_uri":"https://localhost:8080/","height":558},"executionInfo":{"status":"error","timestamp":1635328004254,"user_tz":-180,"elapsed":1058,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}},"outputId":"050650b7-a9dd-4179-9dfb-503b18aac4f3"},"source":["# Challenge 1\n","# ---\n","# Question: Perform linear discriminant analysis to predict the cellular localization sites of proteins\n","# Dataset url = https://www.kaggle.com/imnikhilanand/heart-attack-analysis/data\n","# ---\n","# \n","df = pd.read_csv('https://www.kaggle.com/imnikhilanand/heart-attack-analysis/data')\n","df.head()"],"execution_count":9,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-e423868a41d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.kaggle.com/imnikhilanand/heart-attack-analysis/data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 7, saw 2\n"]}]},{"cell_type":"code","metadata":{"id":"549OtE3KbQGO"},"source":["# Challenge 2\n","# ---\n","# Question: Using the breast cancer wisconsin (diagnostic) dataset perform linear discriminant analysis\n","# Dataset url = http://bit.ly/BreastCancerDataset\n","# --\n","#\n","OUR CODE GOES HERE"],"execution_count":null,"outputs":[]}]}