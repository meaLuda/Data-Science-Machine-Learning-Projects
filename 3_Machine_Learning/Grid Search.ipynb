{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Python Programming: Grid Search","provenance":[{"file_id":"1yFcLacRxAHouQzqkishK0prAKd6gQGT1","timestamp":1634034896338}],"collapsed_sections":["k6mu0__7OjF8","LoZYTKzdNPw1"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"X5ma2Mw6_6Dt"},"source":["# Python Programming: Grid Search"]},{"cell_type":"markdown","metadata":{"id":"k6mu0__7OjF8"},"source":["## Example"]},{"cell_type":"code","metadata":{"id":"tftAsaCN_xbw"},"source":["## Example 1\n","# ---\n","# Perform hyperparameter tuning then predict the quality of wine using Grid Search. \n","# ---\n","# Dataset url = http://bit.ly/TuningDataset\n","# ---\n","# OUR CODE GOES BELOW "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnvF1TbuBFSP"},"source":["# Importing the required libraries\n","# ---\n","#\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6io-7e6WAaLU"},"source":["# Importing our Dataset\n","# ---\n","#\n","dataset = pd.read_csv(\"http://bit.ly/TuningDataset\", sep=';')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fprG-77uCN81"},"source":["# Previewing our Dataset\n","# ---\n","#\n","dataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85-ZhbKfCQNu"},"source":["# Performing Data Preprocessing\n","# ---\n","# \n","X = dataset.iloc[:, 0:11].values\n","y = dataset.iloc[:, 11].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRYfL5brDIMI"},"source":["# Performing Data Preprocessing\n","# ---\n","# \n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfNu9GniC6TL"},"source":["# Scaling our Data\n","# ---\n","# \n","from sklearn.preprocessing import StandardScaler\n","feature_scaler = StandardScaler()\n","X_train = feature_scaler.fit_transform(X_train)\n","X_test = feature_scaler.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jQbqNGppIwCD"},"source":["# Training and Cross Validation\n","# ---\n","# \n","from sklearn.ensemble import RandomForestClassifier\n","classifier = RandomForestClassifier(n_estimators=300, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQz-BvHSJD1C"},"source":["# Training and Cross Validation\n","# ---\n","# Next, to implement cross validation, the cross_val_score method \n","# of the sklearn.model_selection library can be used. \n","# The cross_val_score returns the accuracy for all the folds. \n","# Values for 4 parameters are required to be passed to the cross_val_score class. \n","# The first parameter is estimator which basically specifies \n","# the algorithm that you want to use for cross validation. \n","# The second and third parameters, X and y, contain the X_train and y_train data i.e. features and labels. \n","# Finally the number of folds is passed to the cv parameter as shown in the following code\n","# ---\n","# \n","from sklearn.model_selection import cross_val_score\n","all_accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g288wwhYJEcS"},"source":["# Printing the accuracies returned for five folds \n","# by the cross_val_score method by calling print on all_accuracies\n","# ---\n","#\n","print(all_accuracies.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWXSIEBADSaw"},"source":["# Step 1: Hyperparameters: Getting Started with Grid Search\n","# ---\n","# We create a dictionary of all the parameters and their corresponding \n","# set of values that you want to test for best performance. \n","# The name of the dictionary items corresponds to the parameter name \n","# and the value corresponds to the list of values for the parameter.\n","# As shown grid_param dictionary with three parameters n_estimators, criterion, and bootstrap. \n","# The parameter values that we want to try out are passed in the list. \n","# For instance, in the above script we want to find which value \n","# (out of 100, 300, 500, 800, and 1000) provides the highest accuracy. \n","# Similarly, we want to find which value results in the \n","# highest performance for the criterion parameter: \"gini\" or \"entropy\"? \n","# The Grid Search algorithm basically tries all possible combinations \n","# of parameter values and returns the combination with the highest accuracy. \n","# For instance, in the above case the algorithm will check 20 combinations (5 x 2 x 2 = 20).\n","# ---\n","# \n","grid_param = {\n","    'n_estimators': [100, 300, 500, 800, 1000],\n","    'criterion': ['gini', 'entropy'],\n","    'bootstrap': [True, False]\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BP1h7wGzEFtq"},"source":["# Step 2: Instantiating GridSearchCV object\n","# ---\n","# Once the parameter dictionary is created, the next step \n","# is to create an instance of the GridSearchCV class. \n","# We need to pass values for the estimator parameter, \n","# which basically is the algorithm that you want to execute. \n","# The param_grid parameter takes the parameter dictionary \n","# that we just created as parameter, the scoring parameter \n","# takes the performance metrics, the cv parameter corresponds \n","# to number of folds, which is 5 in our case, and finally \n","# the n_jobs parameter refers to the number of CPU's that we want to use for execution. \n","# A value of -1 for n_jobs parameter means that use all available computing power.\n","# ---\n","# \n","from sklearn.model_selection import GridSearchCV\n","gd_sr = GridSearchCV(estimator=classifier,\n","                     param_grid=grid_param,\n","                     scoring='accuracy',\n","                     cv=5,\n","                     n_jobs=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qdF_J2DHvEP"},"source":["# Step 3: Calling the fit method\n","# ---\n","# Once the GridSearchCV class is initialized, we call the fit method of the class \n","# and pass it the training and test set, as shown in the following code.\n","# The method might take abit of some time to execute. \n","# This is the drawback - GridSearchCV will go through all the intermediate \n","# combinations of hyperparameters which makes grid search computationally very expensive.\n","# ---\n","# \n","gd_sr.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4XzL4IYGh-8"},"source":["# Step 4: Checking the parameters that return the highest accuracy\n","# --- \n","# To do so, we print the sr.best_params_ attribute of the GridSearchCV object, as shown below:\n","# ---\n","# \n","best_parameters = gd_sr.best_params_\n","print(best_parameters)\n","\n","# The result shows that the highest accuracy is achieved \n","# when the n_estimators are 300, bootstrap is True and criterion is \"gini\". \n","# It would be a good idea to add more number of estimators \n","# and see if performance further increases since the highest \n","# allowed value of n_estimators was chosen."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tt0Nj8sDH7XT"},"source":["# Step 5: Finding the obtained accuracy\n","# ---\n","# The last and final step of Grid Search algorithm is \n","# to find the accuracy obtained using the best parameters. \n","# Previously we had a mean accuracy of 64.22%.\n","# To find the best accuracy achieved, we execute the following code:\n","# ---\n","# \n","best_result = gd_sr.best_score_\n","print(best_result)\n","\n","# The accuracy achieved is: 0.6505 of 65.05% which is only slightly better than 64.22%. \n","# To improve this further, it would be good to test values for other parameters \n","# of Random Forest algorithm, such as max_features, max_depth, max_leaf_nodes, etc. \n","# to see if the accuracy further improves or not."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LoZYTKzdNPw1"},"source":["## <font color=\"green\">Challenges</font>"]},{"cell_type":"code","metadata":{"id":"a2Fn-tSkKeWl"},"source":["## Challenge 1\n","# ---\n","# Question: Implement hyperparameter tuning upon creating a model to classify \n","# incomes of persons given the following dataset.\n","# ---\n","# Dataset url = http://bit.ly/HyperParameterTuningDataset\n","# ---\n","# OUR CODE GOES BELOW\n","#"],"execution_count":null,"outputs":[]}]}