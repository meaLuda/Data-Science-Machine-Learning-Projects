{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Python Programming: Lasso Regression","provenance":[{"file_id":"1T6GzdFx-XIZPDqF7r3P34h8kCsmPJVEn","timestamp":1634194151219}],"collapsed_sections":["d2yGbDJ7b2n1","JzyllWEd-hsg","u34RZ3Vf-k5x","MIGQ8Zstb5Gf","Kv0yDYS7b7pJ","18LBTLVdb9Dv","QCJJlsBwb_J3","Qfr0emdhcBAz","o9xRsUOlcC_z"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x5iPaG3CcczE"},"source":["<font color=\"green\">*To start working on this notebook, or any other notebook that we will use in the Moringa Data Science Course, we will need to save our own copy of it. We can do this by clicking File > Save a Copy in Drive. We will then be able to make edits to our own copy of this notebook.*</font>"]},{"cell_type":"markdown","metadata":{"id":"4FPR7vn3bxqL"},"source":["# Python Programming: Lasso Regression"]},{"cell_type":"markdown","metadata":{"id":"d2yGbDJ7b2n1"},"source":["## Examples"]},{"cell_type":"markdown","metadata":{"id":"u34RZ3Vf-k5x"},"source":["### Example 1"]},{"cell_type":"code","metadata":{"id":"elNELEGL-mqT","executionInfo":{"status":"ok","timestamp":1634896835613,"user_tz":-180,"elapsed":9,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Example 1\n","# ---\n","# This is a simple implementation of the lasso regression\n","# ---\n","#\n","\n","# Loading our libraries\n","#\n","from sklearn.linear_model import Lasso\n","from sklearn.datasets import load_boston\n","from sklearn.preprocessing import StandardScaler"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"jr2iIhoe-zMP","executionInfo":{"status":"ok","timestamp":1634896835615,"user_tz":-180,"elapsed":9,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Loading our dataset\n","# \n","boston = load_boston()\n","X = boston.data\n","y = boston.target"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"1m0MR8_b-4ns","executionInfo":{"status":"ok","timestamp":1634896835616,"user_tz":-180,"elapsed":9,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Standadizing our features\n","#\n","scaler = StandardScaler()\n","X_std = scaler.fit_transform(X)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ie_SXa_d_Amd","executionInfo":{"status":"ok","timestamp":1634896853703,"user_tz":-180,"elapsed":912,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}}},"source":["# Fitting Lasso Regression\n","# The hyperparameter, α, lets us control how much we penalize the coefficients, \n","# with higher values of α creating simpler modelers. The ideal value of α\n","# should be tuned like any other hyperparameter. In scikit-learn, α\n","# is set using the alpha parameter.\n","# \n","\n","# Creating lasso regression with alpha value\n","regr = Lasso(alpha=0.5)\n","\n","# Fitting the linear regression\n","model = regr.fit(X_std, y)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMO-TV11cklm","executionInfo":{"status":"ok","timestamp":1634896865832,"user_tz":-180,"elapsed":1374,"user":{"displayName":"Eliud Munyala","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09572496916808459189"}},"outputId":"e17feca5-1b5c-4bcb-86d9-92914b649847"},"source":["model"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Lasso(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=1000,\n","      normalize=False, positive=False, precompute=False, random_state=None,\n","      selection='cyclic', tol=0.0001, warm_start=False)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"JzyllWEd-hsg"},"source":["### Example 2"]},{"cell_type":"code","metadata":{"id":"ZT2gVGqYbqJy"},"source":["# Example 2\n","# ---\n","# Predicting the price using the available attributes while performing lasso regression.\n","# ---\n","# Dataset url = http://bit.ly/DiamondsDataset\n","# ---\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Ui6Yof365iI"},"source":["# Import libraries\n","# \n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwTbDK547AO2"},"source":["# Uploading the dataset\n","#\n","diamonds = pd.read_csv('http://bit.ly/DiamondsDataset')\n","diamonds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqvP-Vxw7QmY"},"source":["# Droping the index\n","#\n","diamonds = diamonds.drop(['Unnamed: 0'], axis=1)\n","diamonds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAg4MkMu7SPz"},"source":["# Printing unique values of text features\n","# \n","print(diamonds.cut.unique())\n","print(diamonds.clarity.unique())\n","print(diamonds.color.unique())\n","\n","# As we can see, there are a finite number of variables, so we can transform these categorical variables to numerical variables."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5-UdrTX7VkX"},"source":["# Importing label encoder\n","# \n","from sklearn.preprocessing import LabelEncoder\n","categorical_features = ['cut', 'color', 'clarity']\n","le = LabelEncoder()\n","\n","# Converting the variables to numerical\n","#\n","for i in range(3):\n","    new = le.fit_transform(diamonds[categorical_features[i]])\n","    diamonds[categorical_features[i]] = new\n","diamonds.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fY3WOsYz7tSr"},"source":["# Before building the models, let's first scale data. \n","# Lasso put constraints on the size of the coefficients associated to each variable. \n","# But, this value depends on the magnitude of each variable and it is therefore \n","# necessary to center and reduce, or standardize, the variables.\n","# \n","\n","# Importing StandardScaler\n","#\n","from sklearn.preprocessing import StandardScaler\n","\n","# Creating features and target matrixes\n","#\n","X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z', 'clarity', 'cut', 'color']]\n","y = diamonds[['price']]\n","\n","# Scaling data \n","#\n","scaler = StandardScaler()\n","scaler.fit(X)\n","X = scaler.transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGv92--o79nq"},"source":["# We can basically build the Lasso model. \n","# But for now, we will train it on the whole dataset and look at an R-squared score and on the model coefficients. \n","# Note, that we are not setting the alpha, it is defined as 1.\n","# \n","\n","# Importing linear models\n","# \n","from sklearn import linear_model\n","from sklearn.metrics import mean_squared_error\n","\n","# Creating lasso object\n","# \n","lasso = linear_model.Lasso() \n","\n","# Fitting the models\n","# \n","lasso.fit(X, y) \n","\n","# Print scores, MSE, and coefficients\n","# \n","print(\"lasso score:\", lasso.score(X, y)) \n","print(\"lasso MSE:\", mean_squared_error(y, lasso.predict(X))) \n","print(\"lasso coef:\", lasso.coef_) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1YSznI18gx2"},"source":["# Splitting the data into training and test sets, building Lasso, \n","# and choosing the regularization parameter with the help of GridSearch. \n","# For that, we have to define the set of parameters for GridSearch. \n","# In this case, a model with the highest R-squared score will give us the best parameters.\n","# \n","\n","# Making necessary imports, split data into training and test sets, and choose a set of parameters \n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)\n","parameters = {'alpha': np.concatenate((np.arange(0.1,2,0.1), np.arange(2, 5, 0.5), np.arange(5, 25, 1)))}\n","\n","linear = linear_model.LinearRegression()\n","lasso = linear_model.Lasso() \n","gridlasso = GridSearchCV(lasso, parameters, scoring ='r2') \n","\n","# Fitting models and print the best parameters, R-squared scores, MSE, and coefficients\n","gridlasso.fit(X_train, y_train) \n","linear.fit(X_train, y_train) \n","print(\"lasso best parameters:\", gridlasso.best_params_) \n","print(\"lasso score:\", gridlasso.score(X_test, y_test))\n","print(\"linear score:\", linear.score(X_test, y_test)) \n","print(\"lasso MSE:\", mean_squared_error(y_test, gridlasso.predict(X_test)))\n","print(\"linear MSE:\", mean_squared_error(y_test, linear.predict(X_test))) \n","print(\"lasso best estimator coef:\", gridlasso.best_estimator_.coef_)\n","print(\"linear coef:\", linear.coef_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4D11jbl877T"},"source":["# Importing library for visualization\n","#\n","import matplotlib.pyplot as plt\n","coefsLasso = [] \n","\n","# Building Lasso for 200 values of alpha and write the coefficients into array\n","# \n","alphasLasso = np.arange (0, 20, 0.1) \n","for i in range(200):\n","    lasso = linear_model.Lasso(alpha=alphasLasso[i])\n","    lasso.fit(X_train, y_train)\n","    coefsLasso.append(lasso.coef_) \n","\n","# Building Lasso coefficient plots\n","# \n","plt.figure(figsize = (16,7))\n","\n","plt.subplot(121)\n","plt.plot(alphasLasso, coefsLasso)\n","plt.title('Lasso coefficients')\n","plt.xlabel('alpha')\n","plt.ylabel('coefs')\n","\n","plt.show()\n","\n","# As we can see, Lasso influences less on the large coefficients, but the small ones Lasso reduces to zeroes. \n","# Therefore Lasso can also be used to determine which features are important \n","# to us and keeps the features that may influence the target variable"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIGQ8Zstb5Gf"},"source":["## <font color=\"green\">Challenges</font>"]},{"cell_type":"markdown","metadata":{"id":"Kv0yDYS7b7pJ"},"source":["### <font color=\"green\">Challenge 1</font>"]},{"cell_type":"code","metadata":{"id":"CnPVr-QVb6uS"},"source":["# Challenge 1\n","# ---\n","# Question: Predict house sales prices for King County given the following dataset applying lasso regression.\n","# ---\n","# Dataset url = http://bit.ly/KCHouseDataset\n","# ---\n","# \n","OUR CODE GOES HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18LBTLVdb9Dv"},"source":["### <font color=\"green\">Challenge 2</font>"]},{"cell_type":"code","metadata":{"id":"UfpYcbiSb-c1"},"source":["# Challenge 2\n","# ---\n","# Question: Build a regression model to predict sales prices given the following house prices dataset.\n","# ---\n","# Dataset source = http://bit.ly/HousePricesDataset\n","# ---\n","# \n","OUR CODE GOES HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCJJlsBwb_J3"},"source":["### <font color=\"green\">Challenge 3</font>"]},{"cell_type":"code","metadata":{"id":"3Ib3KcZ2cAnr"},"source":["# Challenge 3\n","# ---\n","# Question: Given the following dataset, build a regression model to predict sales.\n","# ---\n","# Dataset url = http://bit.ly/AdvertisingDataset\n","# ---\n","#\n","OUR CODE GOES HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qfr0emdhcBAz"},"source":["### <font color=\"green\">Challenge 4</font>"]},{"cell_type":"code","metadata":{"id":"JEfjpgKOcCZh"},"source":["# Challenge 4\n","# ---\n","# Question: Build a model to predict time (in seconds) that a car spends on the test bench given a dataset \n","# that contains an anonymized set of variables, each representing a custom feature in a Mercedes car.\n","# ---\n","# Dataset url = http://bit.ly/MercedesDataset\n","# ---\n","# \n","OUR CODE GOES HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o9xRsUOlcC_z"},"source":["### <font color=\"green\">Challenge 5</font>"]},{"cell_type":"code","metadata":{"id":"DFAy2kj3cFJ7"},"source":["# Challenge 5\n","# ---\n","# Question: Predict the burned area of forest fires, in the northeast region of Portugal, \n","# by using meteorological and other data.\n","# ---\n","# Dataset url = http://bit.ly/ForestFiresDatasetSource\n","# \n","OUR CODE GOES HERE"],"execution_count":null,"outputs":[]}]}